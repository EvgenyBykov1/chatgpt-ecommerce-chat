{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’¬ Chat-Commerce ðŸ›’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "%pip install python-dotenv langchain llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment values\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Turn off unwanted logs\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index E-Commerce Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "# set number of output tokens\n",
    "num_outputs = 2000\n",
    "\n",
    "# OpenAI LLM for indexing\n",
    "llm_index = OpenAI(temperature=0.5, max_tokens=num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index import download_loader\n",
    "\n",
    "# Read the data from CSV\n",
    "PagedCSVReader = download_loader(\"PagedCSVReader\")\n",
    "loader = PagedCSVReader()\n",
    "documents = loader.load_data(file=Path('./data/home-and-garden.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 1815 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 1815 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
    "\n",
    "# set maximum input size\n",
    "max_input_size = 4096\n",
    "# set number of output tokens\n",
    "num_outputs = 2000\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "# set chunk size limit\n",
    "chunk_size_limit = 600 \n",
    "\n",
    "# Index the data and store the embeddings\n",
    "# NOTE: Run this only if you want to re-index\n",
    "llm_predictor = LLMPredictor(llm=llm_index)\n",
    "prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "index_data = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor)\n",
    "index_data.save_to_disk(\"./index/index.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSimpleVectorIndex\n",
    "\n",
    "# Load the embeddings from existing index\n",
    "index_data = GPTSimpleVectorIndex.load_from_disk('./index/index.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "\n",
    "# Initialize OpenAI LLM for chat\n",
    "llm_chat = OpenAIChat(temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "# Create a langchain 'Tool'\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"GPT Index\",\n",
    "        func=lambda q: str(index_data.query(q)),\n",
    "        description=\"useful for when you want to answer questions about products available in the store. The store sells furnitures and items for home and garden\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "> [build_index_from_documents] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [build_index_from_documents] Total embedding token usage: 0 tokens\n",
      "> [build_index_from_documents] Total embedding token usage: 0 tokens\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTListIndex\n",
    "from langchain.agents import initialize_agent\n",
    "from llama_index.langchain_helpers.memory_wrapper import GPTIndexChatMemory\n",
    "\n",
    "# Index to be used for chat history\n",
    "index_chat_history = GPTListIndex([])\n",
    "\n",
    "# chat history\n",
    "memory = GPTIndexChatMemory(\n",
    "    index=index_chat_history, \n",
    "    memory_key=\"chat_history\", \n",
    "    query_kwargs={\"response_mode\": \"compact\"},\n",
    "    # return_source returns source nodes instead of querying index\n",
    "    return_source=True,\n",
    "    # return_messages returns context in message format\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# create an agent\n",
    "agent_chain = initialize_agent(tools, llm=llm_chat, agent=\"conversational-react-description\", memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 0 tokens\n",
      "> [query] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [insert] Total LLM token usage: 0 tokens\n",
      "> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [insert] Total embedding token usage: 0 tokens\n",
      "> [insert] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [insert] Total LLM token usage: 0 tokens\n",
      "> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [insert] Total embedding token usage: 0 tokens\n",
      "> [insert] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Bob! How can I assist you today?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"hi, i am bob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"what are the products available?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"I'm looking for some furniture to use in my garden\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 88 tokens\n",
      "> [query] Total LLM token usage: 88 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 0 tokens\n",
      "> [query] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [query] Total LLM token usage: 155 tokens\n",
      "> [query] Total LLM token usage: 155 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 1 tokens\n",
      "> [query] Total embedding token usage: 1 tokens\n",
      "INFO:root:> [insert] Total LLM token usage: 0 tokens\n",
      "> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [insert] Total embedding token usage: 0 tokens\n",
      "> [insert] Total embedding token usage: 0 tokens\n",
      "INFO:root:> [insert] Total LLM token usage: 0 tokens\n",
      "> [insert] Total LLM token usage: 0 tokens\n",
      "INFO:root:> [insert] Total embedding token usage: 0 tokens\n",
      "> [insert] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nThe context information is referring to a bedside table, which is a type of table typically used for placing items such as lamps, alarm clocks, books, and other items next to a bed.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.run(input=\"Great. Can you pick me one table?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_chain.run(input=\"Yes also show me a picture of it\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
